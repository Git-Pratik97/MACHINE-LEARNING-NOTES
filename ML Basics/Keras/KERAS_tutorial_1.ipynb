{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KERAS_tutorial_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUL8vtv9zC41"
      },
      "source": [
        "**KERAS** is a neural network library which help in developing and evaluating deep learning models. Kers provide high-level API and is built on integration with tensorflow.\n",
        "So downloading tensorflow downloads keras altogether."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvG7BXU1nNqJ"
      },
      "source": [
        "# DATA PREPRATION AND PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LxsBtv1lefo"
      },
      "source": [
        "import numpy as np\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdaQ4lRo7BR"
      },
      "source": [
        "train_labels=[]\n",
        "train_samples=[]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLBzGQKAn-pw"
      },
      "source": [
        "**Example data**\n",
        "\n",
        "1. An experimental drug is tested on individuals from age 13 to 100 in clinical trial.\n",
        "\n",
        "2. Trail had 4200 participants. Half under 65 and half above.\n",
        "3. 95% above 65 had a side effect.\n",
        "4. 95% below 65 had no side effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TGdBxSnlu8"
      },
      "source": [
        "#generate data\n",
        "#1 faced side effect 0 no side effect\n",
        "for i in range(50):\n",
        "  #5% below 65 who faced side effect\n",
        "  random_younger=randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1)\n",
        "\n",
        "  #5% above 65 without side effect\n",
        "  random_older=randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0)\n",
        "\n",
        "for i in range(1000):\n",
        "  #95% below 65 who faced side effect\n",
        "  random_younger=randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(0)\n",
        "\n",
        "  #95% above 65 without side effect\n",
        "  random_older=randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(1)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es5_xu4IqK1I"
      },
      "source": [
        "#convert vector to numpy array\n",
        "train_labels=np.array(train_labels)\n",
        "train_samples=np.array(train_samples)\n",
        "train_labels,train_samples=shuffle(train_labels,train_samples)      #to prevent any oreder if made during data generation"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytclzf7Nqp0b"
      },
      "source": [
        "#scale down data (13-100) to (0-1)    #eariler we did this by divding with 255.0 for images \n",
        "#reshaped since fit trainsform accepts like this\n",
        "# -1 in reshape function is used when you dont know or want to explicitly tell the dimension of that axis. E.g,\n",
        "# If you have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1).\n",
        "\n",
        "scalar=MinMaxScaler(feature_range=(0,1))\n",
        "scaled_train_samples=scalar.fit_transform(train_samples.reshape(-1,1))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p_1US6_sW-s",
        "outputId": "284e49b3-d4f2-4db7-b1a3-1fc143c18224"
      },
      "source": [
        "#checking\n",
        "for i in range(5):\n",
        "  print(scaled_train_samples[i],train_samples[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.63218391] 68\n",
            "[0.82758621] 85\n",
            "[0.05747126] 18\n",
            "[0.62068966] 67\n",
            "[0.36781609] 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogfAdsp7vQyd",
        "outputId": "5c2e50c2-264d-4d05-a611-38e1748cc61f"
      },
      "source": [
        "print(scaled_train_samples.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3WW4vabtafs"
      },
      "source": [
        "#Simple Keras Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq4g_sqnsoum"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkQ2JKoYuaQc"
      },
      "source": [
        "#choice of number of nodes is random.\n",
        "model=Sequential([\n",
        "          Dense(units=16,input_shape=(1,),activation='relu'),\n",
        "          Dense(units=32,activation='relu'),\n",
        "          Dense(units=2,activation='softmax')\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoDebypou2te",
        "outputId": "0c557fdf-7b45-4173-9f91-f795715eb65b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                32        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnDjLGe3u6kQ"
      },
      "source": [
        "#compile specifies detials like optimizer,loss and metrics to define model\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BShrBPHwBfq",
        "outputId": "f912d23b-adaa-4273-a93c-f1abfc82ef28"
      },
      "source": [
        "#model fit is training the data\n",
        "\n",
        "#you may see model not running on full data\n",
        "# IMPORTANT :-\n",
        "# The number total_count/32(132 here) shown during fitting the model is not the training samples; it is the number of batches.\n",
        "# model.fit includes an optional argument batch_size, which, according to the documentation:\n",
        "# If unspecified, batch_size will default to 32.\n",
        "#keep batch_size more so that model runs faster.\n",
        "\n",
        "#verbose=0 no details to show, 2 show full details (0,1,2 allowed values)\n",
        "\n",
        "#more number of epoches better is the accuracy\n",
        "#more number of running this model more better is accuracy, since fit starts from point it left last time.\n",
        "\n",
        "\n",
        "model.fit(x=scaled_train_samples,y=train_labels,epochs=50,verbose=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "66/66 - 14s - loss: 0.6797 - accuracy: 0.4948\n",
            "Epoch 2/50\n",
            "66/66 - 0s - loss: 0.5695 - accuracy: 0.7386\n",
            "Epoch 3/50\n",
            "66/66 - 0s - loss: 0.4634 - accuracy: 0.8371\n",
            "Epoch 4/50\n",
            "66/66 - 0s - loss: 0.3686 - accuracy: 0.9029\n",
            "Epoch 5/50\n",
            "66/66 - 0s - loss: 0.3081 - accuracy: 0.9271\n",
            "Epoch 6/50\n",
            "66/66 - 0s - loss: 0.2765 - accuracy: 0.9333\n",
            "Epoch 7/50\n",
            "66/66 - 0s - loss: 0.2627 - accuracy: 0.9395\n",
            "Epoch 8/50\n",
            "66/66 - 0s - loss: 0.2564 - accuracy: 0.9386\n",
            "Epoch 9/50\n",
            "66/66 - 0s - loss: 0.2509 - accuracy: 0.9410\n",
            "Epoch 10/50\n",
            "66/66 - 0s - loss: 0.2475 - accuracy: 0.9433\n",
            "Epoch 11/50\n",
            "66/66 - 0s - loss: 0.2463 - accuracy: 0.9419\n",
            "Epoch 12/50\n",
            "66/66 - 0s - loss: 0.2440 - accuracy: 0.9452\n",
            "Epoch 13/50\n",
            "66/66 - 0s - loss: 0.2422 - accuracy: 0.9433\n",
            "Epoch 14/50\n",
            "66/66 - 0s - loss: 0.2409 - accuracy: 0.9452\n",
            "Epoch 15/50\n",
            "66/66 - 0s - loss: 0.2402 - accuracy: 0.9443\n",
            "Epoch 16/50\n",
            "66/66 - 0s - loss: 0.2399 - accuracy: 0.9448\n",
            "Epoch 17/50\n",
            "66/66 - 0s - loss: 0.2395 - accuracy: 0.9467\n",
            "Epoch 18/50\n",
            "66/66 - 0s - loss: 0.2387 - accuracy: 0.9452\n",
            "Epoch 19/50\n",
            "66/66 - 0s - loss: 0.2386 - accuracy: 0.9490\n",
            "Epoch 20/50\n",
            "66/66 - 0s - loss: 0.2380 - accuracy: 0.9452\n",
            "Epoch 21/50\n",
            "66/66 - 0s - loss: 0.2365 - accuracy: 0.9452\n",
            "Epoch 22/50\n",
            "66/66 - 0s - loss: 0.2362 - accuracy: 0.9448\n",
            "Epoch 23/50\n",
            "66/66 - 0s - loss: 0.2355 - accuracy: 0.9414\n",
            "Epoch 24/50\n",
            "66/66 - 0s - loss: 0.2348 - accuracy: 0.9462\n",
            "Epoch 25/50\n",
            "66/66 - 0s - loss: 0.2337 - accuracy: 0.9467\n",
            "Epoch 26/50\n",
            "66/66 - 0s - loss: 0.2332 - accuracy: 0.9410\n",
            "Epoch 27/50\n",
            "66/66 - 0s - loss: 0.2344 - accuracy: 0.9433\n",
            "Epoch 28/50\n",
            "66/66 - 0s - loss: 0.2323 - accuracy: 0.9452\n",
            "Epoch 29/50\n",
            "66/66 - 0s - loss: 0.2321 - accuracy: 0.9448\n",
            "Epoch 30/50\n",
            "66/66 - 0s - loss: 0.2312 - accuracy: 0.9452\n",
            "Epoch 31/50\n",
            "66/66 - 0s - loss: 0.2310 - accuracy: 0.9452\n",
            "Epoch 32/50\n",
            "66/66 - 0s - loss: 0.2302 - accuracy: 0.9481\n",
            "Epoch 33/50\n",
            "66/66 - 0s - loss: 0.2295 - accuracy: 0.9462\n",
            "Epoch 34/50\n",
            "66/66 - 0s - loss: 0.2293 - accuracy: 0.9467\n",
            "Epoch 35/50\n",
            "66/66 - 0s - loss: 0.2284 - accuracy: 0.9467\n",
            "Epoch 36/50\n",
            "66/66 - 0s - loss: 0.2293 - accuracy: 0.9433\n",
            "Epoch 37/50\n",
            "66/66 - 0s - loss: 0.2272 - accuracy: 0.9486\n",
            "Epoch 38/50\n",
            "66/66 - 0s - loss: 0.2265 - accuracy: 0.9443\n",
            "Epoch 39/50\n",
            "66/66 - 0s - loss: 0.2265 - accuracy: 0.9476\n",
            "Epoch 40/50\n",
            "66/66 - 0s - loss: 0.2263 - accuracy: 0.9462\n",
            "Epoch 41/50\n",
            "66/66 - 0s - loss: 0.2249 - accuracy: 0.9457\n",
            "Epoch 42/50\n",
            "66/66 - 0s - loss: 0.2242 - accuracy: 0.9476\n",
            "Epoch 43/50\n",
            "66/66 - 0s - loss: 0.2248 - accuracy: 0.9452\n",
            "Epoch 44/50\n",
            "66/66 - 0s - loss: 0.2234 - accuracy: 0.9462\n",
            "Epoch 45/50\n",
            "66/66 - 0s - loss: 0.2235 - accuracy: 0.9481\n",
            "Epoch 46/50\n",
            "66/66 - 0s - loss: 0.2225 - accuracy: 0.9467\n",
            "Epoch 47/50\n",
            "66/66 - 0s - loss: 0.2225 - accuracy: 0.9476\n",
            "Epoch 48/50\n",
            "66/66 - 0s - loss: 0.2215 - accuracy: 0.9467\n",
            "Epoch 49/50\n",
            "66/66 - 0s - loss: 0.2207 - accuracy: 0.9467\n",
            "Epoch 50/50\n",
            "66/66 - 0s - loss: 0.2204 - accuracy: 0.9452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae7a254cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKnxFfmF1R_U"
      },
      "source": [
        "## Validating Model\n",
        "\n",
        "validation dataset is used to see if model is generalized for other data too.\n",
        "Validation is usually part of data taken from training data. Model only learns from training data. \n",
        "**To see model accuracy see validation accuracy.**\n",
        "\n",
        "1. can have seperate validation data\n",
        "2. or can use validation_split=x to have x fraction of training data for validation, x between 0 to 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sx9pi3j1RqB",
        "outputId": "7d42cfc3-1491-4fc4-935c-2cf941aa5d18"
      },
      "source": [
        "model.fit(x=scaled_train_samples,y=train_labels,validation_split=0.1,epochs=50,verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "60/60 - 1s - loss: 0.2200 - accuracy: 0.9481 - val_loss: 0.2188 - val_accuracy: 0.9333\n",
            "Epoch 2/50\n",
            "60/60 - 0s - loss: 0.2191 - accuracy: 0.9460 - val_loss: 0.2166 - val_accuracy: 0.9333\n",
            "Epoch 3/50\n",
            "60/60 - 0s - loss: 0.2192 - accuracy: 0.9466 - val_loss: 0.2184 - val_accuracy: 0.9333\n",
            "Epoch 4/50\n",
            "60/60 - 0s - loss: 0.2186 - accuracy: 0.9460 - val_loss: 0.2188 - val_accuracy: 0.9333\n",
            "Epoch 5/50\n",
            "60/60 - 0s - loss: 0.2179 - accuracy: 0.9487 - val_loss: 0.2235 - val_accuracy: 0.9333\n",
            "Epoch 6/50\n",
            "60/60 - 0s - loss: 0.2181 - accuracy: 0.9450 - val_loss: 0.2206 - val_accuracy: 0.9333\n",
            "Epoch 7/50\n",
            "60/60 - 0s - loss: 0.2184 - accuracy: 0.9450 - val_loss: 0.2165 - val_accuracy: 0.9333\n",
            "Epoch 8/50\n",
            "60/60 - 0s - loss: 0.2165 - accuracy: 0.9481 - val_loss: 0.2145 - val_accuracy: 0.9333\n",
            "Epoch 9/50\n",
            "60/60 - 0s - loss: 0.2160 - accuracy: 0.9466 - val_loss: 0.2142 - val_accuracy: 0.9333\n",
            "Epoch 10/50\n",
            "60/60 - 0s - loss: 0.2166 - accuracy: 0.9466 - val_loss: 0.2170 - val_accuracy: 0.9333\n",
            "Epoch 11/50\n",
            "60/60 - 0s - loss: 0.2162 - accuracy: 0.9487 - val_loss: 0.2151 - val_accuracy: 0.9333\n",
            "Epoch 12/50\n",
            "60/60 - 0s - loss: 0.2158 - accuracy: 0.9450 - val_loss: 0.2139 - val_accuracy: 0.9571\n",
            "Epoch 13/50\n",
            "60/60 - 0s - loss: 0.2163 - accuracy: 0.9439 - val_loss: 0.2119 - val_accuracy: 0.9571\n",
            "Epoch 14/50\n",
            "60/60 - 0s - loss: 0.2159 - accuracy: 0.9460 - val_loss: 0.2139 - val_accuracy: 0.9333\n",
            "Epoch 15/50\n",
            "60/60 - 0s - loss: 0.2136 - accuracy: 0.9466 - val_loss: 0.2209 - val_accuracy: 0.9333\n",
            "Epoch 16/50\n",
            "60/60 - 0s - loss: 0.2139 - accuracy: 0.9466 - val_loss: 0.2135 - val_accuracy: 0.9571\n",
            "Epoch 17/50\n",
            "60/60 - 0s - loss: 0.2132 - accuracy: 0.9503 - val_loss: 0.2139 - val_accuracy: 0.9333\n",
            "Epoch 18/50\n",
            "60/60 - 0s - loss: 0.2137 - accuracy: 0.9481 - val_loss: 0.2125 - val_accuracy: 0.9333\n",
            "Epoch 19/50\n",
            "60/60 - 0s - loss: 0.2131 - accuracy: 0.9471 - val_loss: 0.2163 - val_accuracy: 0.9333\n",
            "Epoch 20/50\n",
            "60/60 - 0s - loss: 0.2128 - accuracy: 0.9455 - val_loss: 0.2128 - val_accuracy: 0.9571\n",
            "Epoch 21/50\n",
            "60/60 - 0s - loss: 0.2129 - accuracy: 0.9471 - val_loss: 0.2090 - val_accuracy: 0.9571\n",
            "Epoch 22/50\n",
            "60/60 - 0s - loss: 0.2120 - accuracy: 0.9492 - val_loss: 0.2097 - val_accuracy: 0.9571\n",
            "Epoch 23/50\n",
            "60/60 - 0s - loss: 0.2130 - accuracy: 0.9460 - val_loss: 0.2129 - val_accuracy: 0.9333\n",
            "Epoch 24/50\n",
            "60/60 - 0s - loss: 0.2108 - accuracy: 0.9476 - val_loss: 0.2122 - val_accuracy: 0.9333\n",
            "Epoch 25/50\n",
            "60/60 - 0s - loss: 0.2106 - accuracy: 0.9492 - val_loss: 0.2088 - val_accuracy: 0.9571\n",
            "Epoch 26/50\n",
            "60/60 - 0s - loss: 0.2105 - accuracy: 0.9476 - val_loss: 0.2074 - val_accuracy: 0.9571\n",
            "Epoch 27/50\n",
            "60/60 - 0s - loss: 0.2119 - accuracy: 0.9476 - val_loss: 0.2097 - val_accuracy: 0.9333\n",
            "Epoch 28/50\n",
            "60/60 - 0s - loss: 0.2099 - accuracy: 0.9471 - val_loss: 0.2096 - val_accuracy: 0.9333\n",
            "Epoch 29/50\n",
            "60/60 - 0s - loss: 0.2096 - accuracy: 0.9487 - val_loss: 0.2214 - val_accuracy: 0.9238\n",
            "Epoch 30/50\n",
            "60/60 - 0s - loss: 0.2098 - accuracy: 0.9492 - val_loss: 0.2147 - val_accuracy: 0.9333\n",
            "Epoch 31/50\n",
            "60/60 - 0s - loss: 0.2096 - accuracy: 0.9476 - val_loss: 0.2130 - val_accuracy: 0.9333\n",
            "Epoch 32/50\n",
            "60/60 - 0s - loss: 0.2082 - accuracy: 0.9471 - val_loss: 0.2066 - val_accuracy: 0.9571\n",
            "Epoch 33/50\n",
            "60/60 - 0s - loss: 0.2086 - accuracy: 0.9471 - val_loss: 0.2060 - val_accuracy: 0.9571\n",
            "Epoch 34/50\n",
            "60/60 - 0s - loss: 0.2091 - accuracy: 0.9476 - val_loss: 0.2124 - val_accuracy: 0.9333\n",
            "Epoch 35/50\n",
            "60/60 - 0s - loss: 0.2081 - accuracy: 0.9476 - val_loss: 0.2086 - val_accuracy: 0.9333\n",
            "Epoch 36/50\n",
            "60/60 - 0s - loss: 0.2076 - accuracy: 0.9508 - val_loss: 0.2072 - val_accuracy: 0.9571\n",
            "Epoch 37/50\n",
            "60/60 - 0s - loss: 0.2075 - accuracy: 0.9492 - val_loss: 0.2135 - val_accuracy: 0.9333\n",
            "Epoch 38/50\n",
            "60/60 - 0s - loss: 0.2073 - accuracy: 0.9466 - val_loss: 0.2049 - val_accuracy: 0.9571\n",
            "Epoch 39/50\n",
            "60/60 - 0s - loss: 0.2072 - accuracy: 0.9487 - val_loss: 0.2112 - val_accuracy: 0.9333\n",
            "Epoch 40/50\n",
            "60/60 - 0s - loss: 0.2086 - accuracy: 0.9444 - val_loss: 0.2042 - val_accuracy: 0.9571\n",
            "Epoch 41/50\n",
            "60/60 - 0s - loss: 0.2072 - accuracy: 0.9492 - val_loss: 0.2068 - val_accuracy: 0.9333\n",
            "Epoch 42/50\n",
            "60/60 - 0s - loss: 0.2067 - accuracy: 0.9487 - val_loss: 0.2089 - val_accuracy: 0.9333\n",
            "Epoch 43/50\n",
            "60/60 - 0s - loss: 0.2063 - accuracy: 0.9497 - val_loss: 0.2094 - val_accuracy: 0.9333\n",
            "Epoch 44/50\n",
            "60/60 - 0s - loss: 0.2058 - accuracy: 0.9455 - val_loss: 0.2057 - val_accuracy: 0.9571\n",
            "Epoch 45/50\n",
            "60/60 - 0s - loss: 0.2063 - accuracy: 0.9487 - val_loss: 0.2055 - val_accuracy: 0.9571\n",
            "Epoch 46/50\n",
            "60/60 - 0s - loss: 0.2054 - accuracy: 0.9487 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
            "Epoch 47/50\n",
            "60/60 - 0s - loss: 0.2053 - accuracy: 0.9471 - val_loss: 0.2114 - val_accuracy: 0.9333\n",
            "Epoch 48/50\n",
            "60/60 - 0s - loss: 0.2066 - accuracy: 0.9497 - val_loss: 0.2125 - val_accuracy: 0.9333\n",
            "Epoch 49/50\n",
            "60/60 - 0s - loss: 0.2065 - accuracy: 0.9471 - val_loss: 0.2023 - val_accuracy: 0.9571\n",
            "Epoch 50/50\n",
            "60/60 - 0s - loss: 0.2062 - accuracy: 0.9481 - val_loss: 0.2017 - val_accuracy: 0.9571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae796a5190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKFBagjCSKKJ"
      },
      "source": [
        "**If accuracy and validation-accuracy are nearly equal then model is fine, if val-accuracy is quite low then accuracy then there can be either underfitting or overfitting. If on running more epoches val-accuracy not improves so overfitting is the problem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01L98wfMUfXJ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtgDyj6p1Ap_"
      },
      "source": [
        "#generating test data\n",
        "test_labels=[]\n",
        "test_samples=[]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM2pY0tkUp4f"
      },
      "source": [
        "#generate test data\n",
        "#1 faced side effect 0 no side effect\n",
        "for i in range(10):\n",
        "  #5% below 65 who faced side effect\n",
        "  random_younger=randint(13,64)\n",
        "  test_samples.append(random_younger)\n",
        "  test_labels.append(1)\n",
        "\n",
        "  #5% above 65 without side effect\n",
        "  random_older=randint(65,100)\n",
        "  test_samples.append(random_older)\n",
        "  test_labels.append(0)\n",
        "\n",
        "for i in range(200):\n",
        "  #95% below 65 who faced side effect\n",
        "  random_younger=randint(13,64)\n",
        "  test_samples.append(random_younger)\n",
        "  test_labels.append(0)\n",
        "\n",
        "  #95% above 65 without side effect\n",
        "  random_older=randint(65,100)\n",
        "  test_samples.append(random_older)\n",
        "  test_labels.append(1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ij6cL-VIWx"
      },
      "source": [
        "# convert vector to numpy array\n",
        "test_labels=np.array(test_labels)\n",
        "test_samples=np.array(test_samples)\n",
        "test_labels,test_samples=shuffle(test_labels,test_samples)  \n",
        "\n",
        "#scale down\n",
        "scalar=MinMaxScaler(feature_range=(0,1))\n",
        "scaled_test_samples=scalar.fit_transform(test_samples.reshape(-1,1))\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od2aInDyXDaT"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy_VoEWuXFk3"
      },
      "source": [
        "np.set_printoptions(precision=3)\n",
        "predictions=model.predict(scaled_test_samples)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igKZ7f8DXaUD",
        "outputId": "eae4eced-180b-443e-cfb6-3b378403de32"
      },
      "source": [
        "for i in predictions:\n",
        "  print(i)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.953 0.047]\n",
            "[0.951 0.049]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.05 0.95]\n",
            "[0.047 0.953]\n",
            "[0.054 0.946]\n",
            "[0.05 0.95]\n",
            "[0.08 0.92]\n",
            "[0.055 0.945]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.059 0.941]\n",
            "[0.039 0.961]\n",
            "[0.039 0.961]\n",
            "[0.953 0.047]\n",
            "[0.048 0.952]\n",
            "[0.953 0.047]\n",
            "[0.097 0.903]\n",
            "[0.044 0.956]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.036 0.964]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.048 0.952]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.053 0.947]\n",
            "[0.097 0.903]\n",
            "[0.051 0.949]\n",
            "[0.953 0.047]\n",
            "[0.063 0.937]\n",
            "[0.403 0.597]\n",
            "[0.403 0.597]\n",
            "[0.953 0.047]\n",
            "[0.036 0.964]\n",
            "[0.403 0.597]\n",
            "[0.953 0.047]\n",
            "[0.044 0.956]\n",
            "[0.953 0.047]\n",
            "[0.954 0.046]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.035 0.965]\n",
            "[0.403 0.597]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.403 0.597]\n",
            "[0.034 0.966]\n",
            "[0.94 0.06]\n",
            "[0.052 0.948]\n",
            "[0.953 0.047]\n",
            "[0.944 0.056]\n",
            "[0.953 0.047]\n",
            "[0.06 0.94]\n",
            "[0.034 0.966]\n",
            "[0.04 0.96]\n",
            "[0.953 0.047]\n",
            "[0.034 0.966]\n",
            "[0.952 0.048]\n",
            "[0.915 0.085]\n",
            "[0.953 0.047]\n",
            "[0.047 0.953]\n",
            "[0.049 0.951]\n",
            "[0.035 0.965]\n",
            "[0.046 0.954]\n",
            "[0.063 0.937]\n",
            "[0.035 0.965]\n",
            "[0.049 0.951]\n",
            "[0.954 0.046]\n",
            "[0.952 0.048]\n",
            "[0.058 0.942]\n",
            "[0.041 0.959]\n",
            "[0.211 0.789]\n",
            "[0.059 0.941]\n",
            "[0.954 0.046]\n",
            "[0.953 0.047]\n",
            "[0.403 0.597]\n",
            "[0.952 0.048]\n",
            "[0.048 0.952]\n",
            "[0.034 0.966]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.074 0.926]\n",
            "[0.403 0.597]\n",
            "[0.045 0.955]\n",
            "[0.953 0.047]\n",
            "[0.053 0.947]\n",
            "[0.04 0.96]\n",
            "[0.211 0.789]\n",
            "[0.953 0.047]\n",
            "[0.035 0.965]\n",
            "[0.051 0.949]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.049 0.951]\n",
            "[0.954 0.046]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.039 0.961]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.051 0.949]\n",
            "[0.953 0.047]\n",
            "[0.043 0.957]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.055 0.945]\n",
            "[0.954 0.046]\n",
            "[0.062 0.938]\n",
            "[0.952 0.048]\n",
            "[0.062 0.938]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.046 0.954]\n",
            "[0.953 0.047]\n",
            "[0.954 0.046]\n",
            "[0.944 0.056]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.811 0.189]\n",
            "[0.953 0.047]\n",
            "[0.045 0.955]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.068 0.932]\n",
            "[0.211 0.789]\n",
            "[0.046 0.954]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.038 0.962]\n",
            "[0.953 0.047]\n",
            "[0.059 0.941]\n",
            "[0.048 0.952]\n",
            "[0.038 0.962]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.037 0.963]\n",
            "[0.403 0.597]\n",
            "[0.045 0.955]\n",
            "[0.953 0.047]\n",
            "[0.055 0.945]\n",
            "[0.954 0.046]\n",
            "[0.038 0.962]\n",
            "[0.403 0.597]\n",
            "[0.953 0.047]\n",
            "[0.944 0.056]\n",
            "[0.953 0.047]\n",
            "[0.052 0.948]\n",
            "[0.046 0.954]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.049 0.951]\n",
            "[0.953 0.047]\n",
            "[0.04 0.96]\n",
            "[0.034 0.966]\n",
            "[0.951 0.049]\n",
            "[0.051 0.949]\n",
            "[0.953 0.047]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.06 0.94]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.937 0.063]\n",
            "[0.034 0.966]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.058 0.942]\n",
            "[0.047 0.953]\n",
            "[0.951 0.049]\n",
            "[0.063 0.937]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.953 0.047]\n",
            "[0.63 0.37]\n",
            "[0.044 0.956]\n",
            "[0.04 0.96]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.944 0.056]\n",
            "[0.045 0.955]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.063 0.937]\n",
            "[0.063 0.937]\n",
            "[0.054 0.946]\n",
            "[0.953 0.047]\n",
            "[0.05 0.95]\n",
            "[0.63 0.37]\n",
            "[0.953 0.047]\n",
            "[0.068 0.932]\n",
            "[0.932 0.068]\n",
            "[0.045 0.955]\n",
            "[0.049 0.951]\n",
            "[0.953 0.047]\n",
            "[0.04 0.96]\n",
            "[0.953 0.047]\n",
            "[0.045 0.955]\n",
            "[0.036 0.964]\n",
            "[0.034 0.966]\n",
            "[0.915 0.085]\n",
            "[0.953 0.047]\n",
            "[0.944 0.056]\n",
            "[0.06 0.94]\n",
            "[0.953 0.047]\n",
            "[0.074 0.926]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.944 0.056]\n",
            "[0.952 0.048]\n",
            "[0.063 0.937]\n",
            "[0.039 0.961]\n",
            "[0.059 0.941]\n",
            "[0.074 0.926]\n",
            "[0.952 0.048]\n",
            "[0.944 0.056]\n",
            "[0.953 0.047]\n",
            "[0.034 0.966]\n",
            "[0.035 0.965]\n",
            "[0.057 0.943]\n",
            "[0.211 0.789]\n",
            "[0.953 0.047]\n",
            "[0.048 0.952]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.048 0.952]\n",
            "[0.954 0.046]\n",
            "[0.951 0.049]\n",
            "[0.953 0.047]\n",
            "[0.034 0.966]\n",
            "[0.055 0.945]\n",
            "[0.059 0.941]\n",
            "[0.953 0.047]\n",
            "[0.06 0.94]\n",
            "[0.058 0.942]\n",
            "[0.953 0.047]\n",
            "[0.042 0.958]\n",
            "[0.037 0.963]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.097 0.903]\n",
            "[0.035 0.965]\n",
            "[0.953 0.047]\n",
            "[0.045 0.955]\n",
            "[0.05 0.95]\n",
            "[0.046 0.954]\n",
            "[0.954 0.046]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.944 0.056]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.06 0.94]\n",
            "[0.062 0.938]\n",
            "[0.953 0.047]\n",
            "[0.04 0.96]\n",
            "[0.038 0.962]\n",
            "[0.953 0.047]\n",
            "[0.08 0.92]\n",
            "[0.954 0.046]\n",
            "[0.062 0.938]\n",
            "[0.035 0.965]\n",
            "[0.051 0.949]\n",
            "[0.06 0.94]\n",
            "[0.074 0.926]\n",
            "[0.954 0.046]\n",
            "[0.953 0.047]\n",
            "[0.059 0.941]\n",
            "[0.63 0.37]\n",
            "[0.041 0.959]\n",
            "[0.043 0.957]\n",
            "[0.953 0.047]\n",
            "[0.045 0.955]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.036 0.964]\n",
            "[0.057 0.943]\n",
            "[0.951 0.049]\n",
            "[0.049 0.951]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.039 0.961]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.037 0.963]\n",
            "[0.953 0.047]\n",
            "[0.042 0.958]\n",
            "[0.937 0.063]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.052 0.948]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.08 0.92]\n",
            "[0.044 0.956]\n",
            "[0.068 0.932]\n",
            "[0.211 0.789]\n",
            "[0.05 0.95]\n",
            "[0.953 0.047]\n",
            "[0.034 0.966]\n",
            "[0.041 0.959]\n",
            "[0.953 0.047]\n",
            "[0.039 0.961]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.04 0.96]\n",
            "[0.038 0.962]\n",
            "[0.05 0.95]\n",
            "[0.953 0.047]\n",
            "[0.062 0.938]\n",
            "[0.05 0.95]\n",
            "[0.054 0.946]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.05 0.95]\n",
            "[0.068 0.932]\n",
            "[0.952 0.048]\n",
            "[0.047 0.953]\n",
            "[0.932 0.068]\n",
            "[0.953 0.047]\n",
            "[0.63 0.37]\n",
            "[0.953 0.047]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.055 0.945]\n",
            "[0.037 0.963]\n",
            "[0.042 0.958]\n",
            "[0.057 0.943]\n",
            "[0.052 0.948]\n",
            "[0.044 0.956]\n",
            "[0.952 0.048]\n",
            "[0.953 0.047]\n",
            "[0.055 0.945]\n",
            "[0.952 0.048]\n",
            "[0.068 0.932]\n",
            "[0.039 0.961]\n",
            "[0.05 0.95]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.05 0.95]\n",
            "[0.034 0.966]\n",
            "[0.403 0.597]\n",
            "[0.036 0.964]\n",
            "[0.953 0.047]\n",
            "[0.048 0.952]\n",
            "[0.403 0.597]\n",
            "[0.034 0.966]\n",
            "[0.063 0.937]\n",
            "[0.953 0.047]\n",
            "[0.038 0.962]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.951 0.049]\n",
            "[0.038 0.962]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.05 0.95]\n",
            "[0.954 0.046]\n",
            "[0.055 0.945]\n",
            "[0.049 0.951]\n",
            "[0.058 0.942]\n",
            "[0.038 0.962]\n",
            "[0.036 0.964]\n",
            "[0.08 0.92]\n",
            "[0.953 0.047]\n",
            "[0.953 0.047]\n",
            "[0.211 0.789]\n",
            "[0.038 0.962]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OBsSc0bYcpN",
        "outputId": "1e96e667-6b1e-436d-a111-63a2eb1e2890"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFa6F0kyXhEc"
      },
      "source": [
        "prediction=np.argmax(predictions,axis=1)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBFOI7EKXu3w",
        "outputId": "706be501-573c-4fb9-d587-1d0222615bd1"
      },
      "source": [
        "for i in prediction:\n",
        "  print(i)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acIQjEJjdisM"
      },
      "source": [
        "#CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAsVhpNYd3AO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "%matplotlib inline"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "EufRtr3kdlom",
        "outputId": "c9752503-6ea1-4479-e8e3-a5c3c4968eed"
      },
      "source": [
        "cm=confusion_matrix(y_true=test_labels,y_pred=prediction)\n",
        "plt.imshow(cm,interpolation='nearest',cmap='Greens')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel(\"Actual Value\",fontsize='13')\t#adds a label in the x axis\n",
        "plt.ylabel(\"Predicted Value\",fontsize='13')\n",
        "\n",
        "thresh=cm.max()/2\n",
        "for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
        "  plt.text(i,j,cm[i,j],horizontalalignment='center')\n",
        "\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "#200+200 correct predictions, 20 wrong predictions\n",
        "# accuracy=400/420=0.9523"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xryKuik2gXK5"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn02NVVQVrA9",
        "outputId": "97b84ff0-e326-4a13-c24d-56d6b7110469"
      },
      "source": [
        "test_loss,test_accuracy=model.evaluate(scaled_test_samples,test_labels)\n",
        "print(\"Accuracy is \"+ str(test_accuracy))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9524\n",
            "Accuracy is 0.9523809552192688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohe047Lcg5dB"
      },
      "source": [
        "#SAVE AND LOAD MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEHj6DFeiZB3"
      },
      "source": [
        "This save function save:\n",
        "\n",
        "\n",
        "*   architecture of model allowing to re-create the model\n",
        "*   the weights of the model\n",
        "*   training configuration(loss,optimizer)\n",
        "*   the state of optimizer,allowing to resume training exactly where you left off\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RAexg38g-Pk",
        "outputId": "4f6a810e-a0d6-498e-d830-0facf7aaa017"
      },
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEeeKZxRhZ6i"
      },
      "source": [
        "#save with extension h5\n",
        "#add file to google drive by drag and drop\n",
        "model.save('medical_trial_data.h5')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnAWBgvJh_IY"
      },
      "source": [
        "#LOAD\n",
        "\n",
        "from keras.models import load_model\n",
        "new_model=load_model('/content/gdrive/MyDrive/medical_trial_data.h5')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l_sLDvFiSTH",
        "outputId": "69c434a2-9223-41ca-8677-572209803c70"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                32        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfLXuivGkQzu",
        "outputId": "db85b914-7872-4ce2-b6dc-308c1e24c2cc"
      },
      "source": [
        "new_model.get_weights()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.365,  0.677,  0.747, -0.521,  0.672,  0.693, -0.215, -0.534,\n",
              "         -0.49 ,  0.73 ,  0.563,  0.272, -0.213, -0.46 ,  0.371, -0.594]],\n",
              "       dtype=float32),\n",
              " array([-0.205, -0.379, -0.366,  0.   , -0.377, -0.388,  0.   ,  0.   ,\n",
              "         0.   , -0.412,  0.1  ,  0.173,  0.   ,  0.   , -0.208,  0.   ],\n",
              "       dtype=float32),\n",
              " array([[ 4.287e-01,  2.753e-01, -3.364e-01,  9.445e-02,  8.691e-02,\n",
              "          1.590e-01,  4.718e-01, -7.934e-02, -1.129e+00,  2.089e-01,\n",
              "         -1.731e+00, -2.242e-01,  2.589e-01,  1.045e+00, -5.740e-01,\n",
              "          2.225e-01,  4.148e-01, -1.518e-01, -2.628e-02,  7.749e-02,\n",
              "          3.529e-01, -1.332e+00, -1.626e-01, -9.703e-02,  3.595e-01,\n",
              "         -6.160e-02, -1.767e+00, -1.196e+00, -1.651e+00, -5.201e-02,\n",
              "         -1.906e+00, -5.801e-02],\n",
              "        [ 5.081e-01,  4.633e-01, -3.524e-01, -3.818e-01,  2.025e-02,\n",
              "          9.548e-02,  4.251e-01, -1.284e-01, -1.112e+00,  4.460e-02,\n",
              "         -1.576e+00,  1.708e-01,  4.346e-01,  8.548e-01, -3.094e-01,\n",
              "          6.959e-02,  5.776e-02,  8.686e-02, -2.245e-01, -1.859e-01,\n",
              "          8.647e-02, -1.263e+00,  3.158e-02,  2.581e-01,  1.393e-01,\n",
              "         -2.381e-01, -1.282e+00, -1.159e+00, -1.047e+00,  1.791e-01,\n",
              "         -1.213e+00, -1.299e-02],\n",
              "        [ 3.261e-01,  8.149e-02, -2.054e-01,  2.221e-01, -3.893e-02,\n",
              "         -2.192e-01,  5.412e-01,  3.441e-03, -3.504e-01,  4.992e-01,\n",
              "         -5.538e-01, -2.250e-01,  2.598e-01, -2.955e-02,  1.226e-01,\n",
              "          3.090e-01,  1.761e-01, -3.327e-01,  4.914e-01, -2.515e-01,\n",
              "          5.671e-01,  1.015e-03,  2.136e-01, -3.584e-01,  4.173e-01,\n",
              "         -2.632e-01, -3.799e-01,  5.037e-03, -6.573e-01,  8.629e-02,\n",
              "         -6.728e-01, -2.015e-01],\n",
              "        [ 2.186e-01, -1.711e-01, -1.490e-01,  1.358e-02, -2.777e-02,\n",
              "          1.793e-01,  4.735e-02,  1.996e-01, -2.583e-01,  3.066e-01,\n",
              "         -2.938e-01, -1.288e-01, -1.278e-01,  3.465e-01,  1.634e-01,\n",
              "          2.682e-01, -4.679e-02,  2.359e-01, -3.509e-01,  5.687e-02,\n",
              "          3.349e-01,  5.621e-02, -2.944e-01, -3.354e-01, -3.418e-03,\n",
              "         -2.710e-01,  1.781e-02,  1.739e-02, -4.964e-02,  2.858e-01,\n",
              "          1.728e-01, -1.372e-01],\n",
              "        [ 3.291e-02, -1.294e-01,  1.677e-01,  2.312e-01,  2.900e-01,\n",
              "          2.897e-01,  3.298e-01, -2.797e-01, -1.208e+00,  3.595e-01,\n",
              "         -1.543e+00, -9.047e-02,  5.656e-01,  6.922e-01, -9.279e-01,\n",
              "          2.662e-01,  3.455e-01, -3.263e-01, -1.161e-01, -1.521e-01,\n",
              "          1.478e-03, -1.461e+00, -2.759e-01,  3.248e-01,  4.318e-01,\n",
              "          1.968e-01, -1.281e+00, -1.453e+00, -1.565e+00, -1.403e-01,\n",
              "         -1.327e+00,  9.446e-02],\n",
              "        [ 6.923e-02,  4.627e-01,  2.346e-01, -1.995e-01, -1.314e-01,\n",
              "         -3.491e-01,  3.267e-01, -3.456e-01, -9.619e-01,  1.352e-01,\n",
              "         -1.305e+00,  3.034e-01,  4.940e-01,  9.041e-01, -6.756e-01,\n",
              "         -3.475e-01,  2.437e-01, -1.058e-02,  4.819e-01, -3.285e-01,\n",
              "          1.080e-01, -1.569e+00, -2.086e-01, -2.499e-01, -4.012e-02,\n",
              "         -3.228e-02, -1.533e+00, -7.518e-01, -1.170e+00,  8.633e-02,\n",
              "         -1.324e+00,  2.930e-01],\n",
              "        [ 1.274e-02,  9.667e-02,  3.100e-01,  7.321e-02, -7.527e-02,\n",
              "         -8.313e-02, -2.368e-01, -2.773e-01,  1.148e-01, -2.234e-01,\n",
              "          4.167e-02,  1.229e-03, -1.887e-01,  2.880e-01, -1.298e-01,\n",
              "          2.448e-01,  2.792e-01,  3.324e-01,  1.796e-01,  1.835e-02,\n",
              "         -2.072e-02, -1.077e-01,  2.750e-01, -2.927e-01, -2.081e-01,\n",
              "         -3.163e-02, -9.104e-02,  2.948e-01,  8.723e-02, -1.513e-01,\n",
              "          3.488e-01,  3.500e-01],\n",
              "        [-2.649e-01, -6.038e-04, -1.654e-02,  1.042e-01,  1.278e-02,\n",
              "         -1.482e-01, -1.460e-01, -1.841e-01,  1.042e-01, -3.033e-01,\n",
              "         -6.746e-02,  2.942e-01,  2.017e-02,  1.974e-01,  1.920e-01,\n",
              "          1.336e-01, -3.451e-01, -2.569e-01, -2.049e-01, -3.116e-02,\n",
              "          1.997e-01,  1.269e-01, -1.372e-01, -1.130e-01,  2.306e-01,\n",
              "          5.679e-02, -1.749e-01,  1.702e-01,  4.855e-02, -2.086e-02,\n",
              "          3.177e-01, -3.412e-01],\n",
              "        [ 1.857e-02,  1.525e-01, -3.310e-01, -2.291e-01,  8.724e-02,\n",
              "          2.330e-01, -3.011e-02,  2.314e-01,  1.849e-01, -2.797e-01,\n",
              "         -3.308e-01,  1.827e-01, -1.989e-01, -2.279e-01,  2.513e-01,\n",
              "          9.171e-02,  3.272e-01,  3.568e-02,  2.329e-02,  5.395e-02,\n",
              "          1.439e-01,  1.335e-01,  2.939e-01,  2.219e-01,  1.841e-01,\n",
              "         -8.317e-02, -1.053e-01, -1.517e-01, -3.134e-02,  1.937e-01,\n",
              "          2.106e-01, -2.408e-01],\n",
              "        [ 4.109e-01,  4.792e-01, -1.206e-01,  2.649e-01,  1.366e-01,\n",
              "          3.051e-01,  1.244e-01, -2.860e-01, -1.209e+00,  1.784e-01,\n",
              "         -1.155e+00, -1.641e-01,  3.993e-01,  7.228e-01, -3.163e-01,\n",
              "         -2.446e-01,  3.399e-02,  2.460e-01,  2.729e-01,  3.195e-01,\n",
              "          1.325e-01, -1.089e+00,  5.823e-03, -2.788e-01,  3.296e-02,\n",
              "         -3.504e-01, -1.394e+00, -8.877e-01, -7.534e-01, -2.702e-01,\n",
              "         -1.202e+00,  9.348e-02],\n",
              "        [ 5.932e-01,  6.009e-01,  1.498e-02, -5.160e-02, -2.603e-01,\n",
              "         -3.624e-01,  4.988e-01, -1.294e-01,  2.116e-01,  1.137e-01,\n",
              "          1.963e-01,  2.278e-01,  5.669e-01,  8.902e-02,  1.301e-01,\n",
              "         -2.237e-01,  3.835e-03, -2.858e-01,  1.657e-01, -1.111e-01,\n",
              "          4.504e-01,  2.484e-01, -3.107e-01,  2.917e-02,  4.439e-01,\n",
              "          2.689e-01, -5.595e-02,  1.785e-02, -1.461e-01, -2.553e-01,\n",
              "          5.703e-02, -2.152e-03],\n",
              "        [ 4.063e-01,  5.176e-01, -3.428e-01, -3.553e-01, -2.643e-01,\n",
              "          4.228e-02,  2.677e-02, -1.713e-01, -5.383e-02,  4.914e-01,\n",
              "          3.377e-01, -2.707e-01,  2.836e-01, -8.014e-02, -1.054e-01,\n",
              "         -2.165e-01,  5.386e-01,  1.814e-01,  2.554e-01, -6.720e-03,\n",
              "         -1.322e-01, -2.588e-01, -3.405e-03, -5.651e-02,  1.979e-01,\n",
              "         -2.662e-01,  4.347e-01,  6.822e-02,  4.133e-01,  2.166e-01,\n",
              "          2.528e-01, -2.730e-01],\n",
              "        [-2.676e-01,  2.911e-01,  1.491e-01, -1.371e-01, -3.438e-01,\n",
              "          3.359e-02, -1.848e-01, -2.945e-01,  1.338e-01,  1.449e-01,\n",
              "         -3.605e-02,  5.755e-02, -5.524e-02, -6.015e-02, -1.496e-01,\n",
              "         -3.383e-01,  9.386e-02,  3.452e-01,  3.518e-02,  5.260e-02,\n",
              "         -2.221e-01, -2.909e-05,  2.491e-01,  2.534e-01, -2.798e-01,\n",
              "          3.350e-01,  3.190e-01,  2.177e-01, -9.452e-02, -1.051e-01,\n",
              "         -2.340e-02, -2.598e-01],\n",
              "        [ 2.055e-01,  3.514e-01, -3.401e-02, -3.097e-01,  1.755e-01,\n",
              "         -2.029e-01, -6.356e-03,  9.547e-02,  8.853e-02, -1.779e-01,\n",
              "          6.494e-02,  9.862e-02,  2.266e-01,  2.530e-01,  3.009e-01,\n",
              "          1.812e-01,  1.545e-01, -1.405e-01, -2.136e-01,  1.313e-01,\n",
              "         -2.845e-01, -1.278e-01,  2.568e-01, -3.207e-01, -7.652e-02,\n",
              "         -1.428e-01,  2.938e-01,  1.032e-01,  3.409e-01,  1.768e-01,\n",
              "          2.500e-01,  1.731e-01],\n",
              "        [ 4.340e-01,  6.987e-02, -2.923e-01, -3.403e-01,  1.272e-02,\n",
              "          2.365e-01,  4.394e-01, -2.671e-01, -1.525e+00, -1.735e-02,\n",
              "         -1.907e+00,  2.024e-01,  1.373e-02,  8.476e-01, -8.536e-01,\n",
              "         -2.670e-03,  1.435e-01, -8.651e-02,  5.054e-02, -1.978e-02,\n",
              "          5.468e-01, -1.881e+00,  2.403e-01, -1.324e-01,  4.197e-01,\n",
              "         -1.569e-01, -1.565e+00, -1.182e+00, -1.481e+00, -1.119e-01,\n",
              "         -1.596e+00,  2.632e-01],\n",
              "        [ 2.434e-01, -2.285e-01,  1.514e-01,  2.577e-03, -1.367e-01,\n",
              "          3.047e-01,  5.544e-02,  2.749e-01,  3.361e-01, -3.401e-01,\n",
              "         -1.837e-01,  2.752e-01, -1.879e-01,  1.447e-01,  1.823e-01,\n",
              "          7.514e-02,  3.125e-01, -1.100e-01,  1.847e-01,  3.315e-01,\n",
              "         -2.288e-01,  1.648e-01,  6.501e-02, -2.287e-01,  3.123e-01,\n",
              "         -1.988e-01, -1.493e-01, -1.117e-01, -2.351e-01,  3.093e-01,\n",
              "          4.409e-02,  8.236e-02]], dtype=float32),\n",
              " array([-0.101,  0.072,  0.   , -0.017, -0.03 , -0.01 , -0.147,  0.   ,\n",
              "         0.207,  0.026,  0.178, -0.02 , -0.078, -0.316,  0.089, -0.006,\n",
              "         0.198, -0.03 ,  0.056,  0.   , -0.094,  0.269,  0.   , -0.034,\n",
              "        -0.023, -0.023,  0.222,  0.192,  0.255, -0.023,  0.254, -0.041],\n",
              "       dtype=float32),\n",
              " array([[-0.542, -0.055],\n",
              "        [-0.564,  0.229],\n",
              "        [ 0.036, -0.19 ],\n",
              "        [ 0.255,  0.015],\n",
              "        [-0.204, -0.389],\n",
              "        [ 0.167, -0.125],\n",
              "        [-0.226,  0.225],\n",
              "        [-0.224,  0.096],\n",
              "        [ 1.084, -1.377],\n",
              "        [-0.511,  0.215],\n",
              "        [ 1.399, -0.842],\n",
              "        [ 0.304, -0.323],\n",
              "        [-0.212,  0.135],\n",
              "        [ 1.031, -0.917],\n",
              "        [ 0.672, -0.723],\n",
              "        [ 0.225,  0.378],\n",
              "        [ 0.123,  0.592],\n",
              "        [-0.114,  0.12 ],\n",
              "        [-0.41 ,  0.67 ],\n",
              "        [ 0.269, -0.378],\n",
              "        [-0.255,  0.457],\n",
              "        [ 1.191, -1.029],\n",
              "        [-0.159, -0.099],\n",
              "        [ 0.136,  0.365],\n",
              "        [-0.162,  0.602],\n",
              "        [ 0.14 ,  0.345],\n",
              "        [ 0.694, -1.394],\n",
              "        [ 1.067, -0.684],\n",
              "        [ 1.273, -0.629],\n",
              "        [-0.234, -0.032],\n",
              "        [ 1.372, -0.62 ],\n",
              "        [ 0.318, -0.316]], dtype=float32),\n",
              " array([-0.175,  0.175], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6NAnZfwkeAg"
      },
      "source": [
        "#MODEL TO JSON\n",
        "\n",
        "if u want to save only architecture of model and not it's weights or its training configuration, you can use following function to save the architecture only.\n",
        "\n",
        "So compiling and training has to be done again\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DrwCBYMkdXr"
      },
      "source": [
        "json_string=model.to_json()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "A_An7oRQk-iz",
        "outputId": "a23bd270-43d2-4a0e-8dc4-4f3e8b769416"
      },
      "source": [
        "json_string"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.5.0\", \"backend\": \"tensorflow\"}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSTBav8lBJJ"
      },
      "source": [
        "#model reconstruction from json\n",
        "from keras.models import model_from_json\n",
        "model_architecture=model_from_json(json_string)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHvwBQJZlc0k",
        "outputId": "ed92bcaf-566c-4fda-d07f-889ec5aca803"
      },
      "source": [
        "model_architecture.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                32        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8poNkSbnlgr5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}